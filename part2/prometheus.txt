Step 1 involves the installation of helm and configuration of the necessary repositories. 
For MacOS, the installation of helm was performed to simplify the prometheus installation process. 
The "brew install helm" command was used to install helm via Homebrew.
Once installed, the subsequent task was to install prometheus using helm, following the deprecation of some prometheus repositories. 
In this case, the latest repository was used. 
The prometheus-community chart was added to the installed helm using the command "helm repo add prometheus-community https://prometheus-community.github.io/helm-charts", as well as the stable version using "helm repo add stable https://charts.helm.sh/stable". 
The helm repositories were then updated with "helm repo update". This marked the completion of step 1.




Step 2 involves the installation of prometheus-server using helm. 
It was observed that the repositories chart contained several pods that could be installed. 
The prometheus-server was located in the prometheus-community/prometheus path, therefore the following command was used to install it: "helm install prometheus prometheus-community/prometheus". 
After the installation, the pods were checked and all the required pods were successfully added.

kubectl get pods
kubectl get services

NAME                                                READY   STATUS    RESTARTS        AGE
assignment3-django-deploy-9555dd69-ltq8f            1/1     Running   1 (9m31s ago)   7d21h
mysql-container-85867fdf9d-lpvlk                    1/1     Running   1 (9m31s ago)   7d21h
prometheus-alertmanager-0                           1/1     Running   0               64s
prometheus-kube-state-metrics-7f6769f7c6-fsjjp      1/1     Running   0               64s
prometheus-prometheus-node-exporter-wmbcg           1/1     Running   0               64s
prometheus-prometheus-pushgateway-684dc6674-872bf   1/1     Running   0               64s
prometheus-server-b9bdb5877-c4xlq                   2/2     Running   0               64s
proxy-84787764d7-4mpxq                              1/1     Running   2 (8m56s ago)   7d21h
NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
assignment3-django-service            NodePort    10.98.204.128    <none>        8000:30928/TCP   7d21h
kubernetes                            ClusterIP   10.96.0.1        <none>        443/TCP          13d
mysql-service                         ClusterIP   10.103.221.21    <none>        3306/TCP         7d21h
prometheus-alertmanager               ClusterIP   10.101.118.188   <none>        9093/TCP         64s
prometheus-alertmanager-headless      ClusterIP   None             <none>        9093/TCP         64s
prometheus-kube-state-metrics         ClusterIP   10.100.26.253    <none>        8080/TCP         64s
prometheus-prometheus-node-exporter   ClusterIP   10.103.144.118   <none>        9100/TCP         64s
prometheus-prometheus-pushgateway     ClusterIP   10.109.123.3     <none>        9091/TCP         64s
prometheus-server                     ClusterIP   10.103.154.212   <none>        80/TCP           64s
proxy-service                         NodePort    10.105.105.6     <none>        8080:31764/TCP   7d21h

Use the command below to run the prometheus gui:

kubectl port-forward deployment/prometheus-server 9090
However, it was observed that Prometheus was not receiving information from the Django service established in the previous section. 
Consequently, in the third step, adjustments were made to the Prometheus server's configuration map, enabling it to monitor the Django service successfully.


Step 3 involved configuring the Prometheus server to monitor the Django service. When setting up the Prometheus server using Helm, a configmap called "prometheus-server" was automatically generated. 
Rather than creating a new configmap, the target in the existing configmap was modified using the following command:
kubectl edit configmap prometheus-server -o yaml
Now the configmao could be edited with vim in the terminal.
To make the prometheus listen to the django service automatically, only one field should be configured.

In line 18 after     
- job_name: prometheus
      static_configs:
      - targets:
The original target was localhost:9090 only, which means in the default case the prometheus-server would only monitoring itself. Hence we add the service directed to the django service:
We add the line:

- proxy-service:8080

Now it becomes
- job_name: prometheus
  static_configs:
  - targets:
    - localhost:9090
    - proxy-service:8080
In the previous sections, a proxy service was established, which was utilized as a proxy for the "assignment3-django-service". Once this was added, the vim editor's ":wq" command was used to save and exit the edit.

It was observed that the default interval for updating the configmap was one minute, and for this assignment, no changes were made to this setting. Consequently, we should wait for approximately one minute for the monitoring service of the Django service to be fully functional.

For the submission, the configmap should also be output as a .yaml file in the "part2/k8" directory using the following command:

"kubectl get configmaps prometheus-server -o yaml > part2/k8/prometheus-configmap.yaml"

Step 4 involved testing the monitoring setup.

It is essential to start the Django service before configuring Prometheus to monitor it. To launch the Django GUI, use the command:

"kubectl port-forward service/proxy-service 7080:8080"

Then, to start the Prometheus GUI, use the same method as starting the Django service, and execute the command:

"kubectl port-forward deployment/prometheus-server 9090"

Afterward, to search for the 404 counter added in Part 2.2, locate the "database_error_return_404" field added in the views.py file.

To conduct further testing, use "localhost:7080" to access the Django GUI and perform different actions such as registering, logging in, and buying/gifting/using a card. Then, execute the command in the Prometheus GUI with the corresponding field name, such as "python_request_r_posts," "python_request_l_posts," or "python_request_g_posts." The number of counters should increase in response to the performed actions.